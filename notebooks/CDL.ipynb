{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.warp\n",
    "import geopyspark as gps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from geonotebook.wrappers import TMSRasterData\n",
    "from osgeo import osr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fa9345b71d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = gps.geopyspark_conf(\"yarn-client\", \"CDL Ingest\")\n",
    "conf.set(\"spark.dynamicAllocation.enabled\", False)\n",
    "conf.set(\"spark.hadoop.yarn.timeline-service.enabled\", False)\n",
    "#conf.set(\"spark.kryo.unsafe\", True)\n",
    "#conf.set(\"spark.rdd.compress\", True)\n",
    "conf.set(\"spark.yarn.executor.memoryOverhead\", \"3G\")\n",
    "conf.set(\"spark.yarn.driver.memoryOverhead\", \"3G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uri = 's3://datahub-rawdata-us-east-1/cdl/CDLS_2016_30m.tif'\n",
    "# uri = '/tmp/CDLS_2016_30m.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raster_layer = gps.geotiff.get(gps.LayerType.SPATIAL, uri, num_partitions=(32*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 12, ip-172-31-52-81.ec2.internal, executor 2): java.lang.InstantiationError: java.lang.reflect.InvocationHandler\n\tat sun.reflect.GeneratedSerializationConstructorAccessor373.newInstance(Unknown Source)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator.newInstance(SunReflectionFactoryInstantiator.java:56)\n\tat com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1090)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:570)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:546)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:41)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)\n\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.InstantiationError: java.lang.reflect.InvocationHandler\n\tat sun.reflect.GeneratedSerializationConstructorAccessor373.newInstance(Unknown Source)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator.newInstance(SunReflectionFactoryInstantiator.java:56)\n\tat com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1090)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:570)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:546)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:41)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)\n\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d19aaf6b32a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraster_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/hadoop/.local/lib/python3.4/site-packages/geopyspark/geotrellis/layer.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o37.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 12, ip-172-31-52-81.ec2.internal, executor 2): java.lang.InstantiationError: java.lang.reflect.InvocationHandler\n\tat sun.reflect.GeneratedSerializationConstructorAccessor373.newInstance(Unknown Source)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator.newInstance(SunReflectionFactoryInstantiator.java:56)\n\tat com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1090)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:570)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:546)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:41)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)\n\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.InstantiationError: java.lang.reflect.InvocationHandler\n\tat sun.reflect.GeneratedSerializationConstructorAccessor373.newInstance(Unknown Source)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.objenesis.instantiator.sun.SunReflectionFactoryInstantiator.newInstance(SunReflectionFactoryInstantiator.java:56)\n\tat com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1090)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:570)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:546)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:551)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:41)\n\tat com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)\n\tat com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:790)\n\tat org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:244)\n\tat org.apache.spark.serializer.DeserializationStream.readValue(Serializer.scala:159)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:189)\n\tat org.apache.spark.serializer.DeserializationStream$$anon$2.getNext(Serializer.scala:186)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "raster_layer.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoNotebook + GeoPySpark",
   "language": "python",
   "name": "geonotebook3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
