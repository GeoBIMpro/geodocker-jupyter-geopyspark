{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.features\n",
    "import rasterio.warp\n",
    "import geopyspark as gps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from geonotebook.wrappers import TMSRasterData\n",
    "from osgeo import osr\n",
    "\n",
    "import os\n",
    "import math\n",
    "import boto3\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = gps.geopyspark_conf(\"yarn-client\", \"SRTM Ingest\") \\\n",
    "          .set(\"spark.dynamicAllocation.enabled\", \"false\") \\\n",
    "          .set(\"spark.executor.instances\", \"50\") \\\n",
    "          .set(\"spark.executor.memory\", \"9472M\") \\\n",
    "          .set(\"spark.executor.cores\", \"4\") \\\n",
    "          .set(\"spark.ui.enabled\",\"true\") \\\n",
    "          .set(\"spark.hadoop.yarn.timeline-service.enabled\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "def get_raster_s3_objects(bucket, prefix, extension=\"hgt\"):\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "    results = []\n",
    "    for page in page_iterator:\n",
    "        for item in page['Contents']:\n",
    "            if item['Key'].endswith(extension):\n",
    "                results.append(item)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_names = get_raster_s3_objects(\"mrgeo-source\", \"srtm-v3-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14265\n",
      "['N00E006.hgt', 'N00E009.hgt', 'N00E010.hgt', 'N00E011.hgt', 'N00E012.hgt', 'N00E013.hgt', 'N00E014.hgt', 'N00E015.hgt', 'N00E016.hgt', 'N00E017.hgt']\n"
     ]
    }
   ],
   "source": [
    "file_names = list(map(lambda d: d['Key'][len('srtm-v3-30/'):], object_names))\n",
    "print(len(file_names))\n",
    "print(file_names[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metadata(uri):\n",
    "    import rasterio\n",
    "    from osgeo import osr\n",
    "    import os\n",
    "    \n",
    "    if \"GDAL_DATA\" not in os.environ:\n",
    "        os.environ[\"GDAL_DATA\"]=\"/usr/local/lib64/python3.4/site-packages/fiona/gdal_data\"\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(uri) as dataset:\n",
    "            bounds = dataset.bounds\n",
    "            height = dataset.height\n",
    "            width = dataset.width\n",
    "            crs = dataset.get_crs()\n",
    "            srs = osr.SpatialReference()\n",
    "            srs.ImportFromWkt(crs.wkt)\n",
    "            proj4 = srs.ExportToProj4()\n",
    "            # ws = [w for (ij, w) in dataset.block_windows()]\n",
    "            tile_cols = math.floor((width - 1) / 512) * 512 # XXX\n",
    "            tile_rows = math.floor((height - 1) / 512) * 512 # XXX\n",
    "            ws = [((x, x + 512), (y, y + 512)) for x in range(0, tile_cols, 512) for y in range(0, tile_rows, 512)]\n",
    "    except:\n",
    "            ws = []\n",
    "            \n",
    "    def windows(uri, ws):\n",
    "        for w in ws:\n",
    "            ((row_start, row_stop), (col_start, col_stop)) = w\n",
    "\n",
    "            left  = bounds.left + (bounds.right - bounds.left)*(float(col_start)/width)\n",
    "            right = bounds.left + (bounds.right - bounds.left)*(float(col_stop)/ width)\n",
    "            bottom = bounds.top + (bounds.bottom - bounds.top)*(float(row_stop)/height)\n",
    "            top = bounds.top + (bounds.bottom - bounds.top)*(float(row_start)/height)\n",
    "            extent = gps.Extent(left,bottom,right,top)\n",
    "                \n",
    "            new_line = {}\n",
    "            new_line['uri'] = uri\n",
    "            new_line['window'] = w\n",
    "            new_line['projected_extent'] = gps.ProjectedExtent(extent=extent, proj4=proj4)\n",
    "            yield new_line\n",
    "    \n",
    "    return [i for i in windows(uri, ws)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(line):\n",
    "    import rasterio\n",
    "    \n",
    "    new_line = line.copy()\n",
    "\n",
    "    with rasterio.open(line['uri']) as dataset:\n",
    "        new_line['data'] = dataset.read(1, window=line['window'])\n",
    "        new_line.pop('window')\n",
    "        new_line.pop('uri')\n",
    "    \n",
    "    return new_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filename_to_data(filename):\n",
    "    import os\n",
    "    \n",
    "    full_filename = \"/vsicurl/https://s3.amazonaws.com/mrgeo-source/srtm-v3-30/{}\".format(filename)\n",
    "    data = [get_data(line) for line in get_metadata(full_filename)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698985\n"
     ]
    }
   ],
   "source": [
    "rdd0 = sc.parallelize(file_names)\n",
    "rdd1 = rdd0.flatMap(filename_to_data)\n",
    "print(rdd1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd1.groupBy(lambda line: line['projected_extent']) # XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_tiles(line):\n",
    "    projected_extent = line[0]\n",
    "    array = np.array([l['data'] for l in line[1]])\n",
    "    tile = gps.Tile.from_numpy_array(array, no_data_value=0)\n",
    "    return (projected_extent, tile)\n",
    "\n",
    "def interesting_tile(line):\n",
    "    [pe, tile] = line\n",
    "    return (np.sum(tile[0][0]) != 0)\n",
    "\n",
    "def square_tile(line):\n",
    "    [pe, tile] = line\n",
    "    return tile[0][0].shape == (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rdd3 = rdd2.repartition(50 * 1024).map(make_tiles).filter(square_tile)\n",
    "rdd3 = rdd2.repartition(50 * 1024).map(make_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raster_layer = gps.RasterLayer.from_numpy_rdd(gps.LayerType.SPATIAL, rdd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tiled_raster_layer = raster_layer.tile_to_layout(layout = gps.GlobalLayout(), target_crs=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyramid = tiled_raster_layer.pyramid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o143.writeSpatial.\n: geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = \"srtm-geopyspark\", zoom = 12)\n\tat geotrellis.spark.io.s3.S3LayerWriter._write(S3LayerWriter.scala:123)\n\tat geotrellis.spark.io.s3.S3LayerWriter._write(S3LayerWriter.scala:49)\n\tat geotrellis.spark.io.LayerWriter$class.write(LayerWriter.scala:153)\n\tat geotrellis.spark.io.s3.S3LayerWriter.write(S3LayerWriter.scala:49)\n\tat geopyspark.geotrellis.io.LayerWriterWrapper.writeSpatial(LayerWriterWrapper.scala:91)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 193 (coalesce at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to ip-172-31-21-248.ec2.internal/172.31.21.248:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:361)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:336)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\nCaused by: java.io.IOException: Failed to connect to ip-172-31-21-248.ec2.internal/172.31.21.248:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)\n\tat org.apache.spark.network.shuffle.ExternalShuffleClient$1.createAndStart(ExternalShuffleClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: ip-172-31-21-248.ec2.internal/172.31.21.248:7337\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)\n\t... 2 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1492)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1492)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1717)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1675)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1664)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)\n\tat geotrellis.spark.io.s3.S3RDDWriter$class.update(S3RDDWriter.scala:83)\n\tat geotrellis.spark.io.s3.S3RDDWriter$.update(S3RDDWriter.scala:161)\n\tat geotrellis.spark.io.s3.S3RDDWriter$class.write(S3RDDWriter.scala:54)\n\tat geotrellis.spark.io.s3.S3RDDWriter$.write(S3RDDWriter.scala:161)\n\tat geotrellis.spark.io.s3.S3LayerWriter._write(S3LayerWriter.scala:127)\n\t... 15 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-069c507bd794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpyramid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mgps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://geotrellis-test/dg-srtm/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"srtm-geopyspark\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/geopyspark/geotrellis/catalog.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(uri, layer_name, tiled_raster_layer, index_strategy, time_unit, time_resolution, store)\u001b[0m\n\u001b[1;32m    286\u001b[0m         writer.writeSpatial(layer_name,\n\u001b[1;32m    287\u001b[0m                             \u001b[0mtiled_raster_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrdd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                             IndexingMethod(index_strategy).value)\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtiled_raster_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLayerType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSPACETIME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o143.writeSpatial.\n: geotrellis.spark.io.package$LayerWriteError: Failed to write Layer(name = \"srtm-geopyspark\", zoom = 12)\n\tat geotrellis.spark.io.s3.S3LayerWriter._write(S3LayerWriter.scala:123)\n\tat geotrellis.spark.io.s3.S3LayerWriter._write(S3LayerWriter.scala:49)\n\tat geotrellis.spark.io.LayerWriter$class.write(LayerWriter.scala:153)\n\tat geotrellis.spark.io.s3.S3LayerWriter.write(S3LayerWriter.scala:49)\n\tat geopyspark.geotrellis.io.LayerWriterWrapper.writeSpatial(LayerWriterWrapper.scala:91)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: ShuffleMapStage 193 (coalesce at NativeMethodAccessorImpl.java:0) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: Failed to connect to ip-172-31-21-248.ec2.internal/172.31.21.248:7337\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:361)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:336)\n\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:54)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:504)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:328)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:269)\nCaused by: java.io.IOException: Failed to connect to ip-172-31-21-248.ec2.internal/172.31.21.248:7337\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)\n\tat org.apache.spark.network.shuffle.ExternalShuffleClient$1.createAndStart(ExternalShuffleClient.java:105)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)\n\tat org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:171)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: io.netty.channel.AbstractChannel$AnnotatedNoRouteToHostException: No route to host: ip-172-31-21-248.ec2.internal/172.31.21.248:7337\n\tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n\tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:257)\n\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:291)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:640)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:575)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:489)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:451)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)\n\t... 2 more\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1505)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1492)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1492)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1263)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1717)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1675)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1664)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1965)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:926)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:924)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:924)\n\tat geotrellis.spark.io.s3.S3RDDWriter$class.update(S3RDDWriter.scala:83)\n\tat geotrellis.spark.io.s3.S3RDDWriter$.update(S3RDDWriter.scala:161)\n\tat geotrellis.spark.io.s3.S3RDDWriter$class.write(S3RDDWriter.scala:54)\n\tat geotrellis.spark.io.s3.S3RDDWriter$.write(S3RDDWriter.scala:161)\n\tat geotrellis.spark.io.s3.S3LayerWriter._write(S3LayerWriter.scala:127)\n\t... 15 more\n"
     ]
    }
   ],
   "source": [
    "for layer in pyramid.levels.values():\n",
    "    gps.write(\"s3://geotrellis-test/dg-srtm/\", \"srtm-geopyspark\", layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GeoNotebook + GeoPySpark",
   "language": "python",
   "name": "geonotebook3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
